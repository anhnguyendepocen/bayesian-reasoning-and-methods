<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Considering Prior Distributions | An Introduction to Bayesian Reasoning and Methods</title>
  <meta name="description" content="This textbook presents an introduction to Bayesian reasoning and methods" />
  <meta name="generator" content="bookdown 0.20.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Considering Prior Distributions | An Introduction to Bayesian Reasoning and Methods" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This textbook presents an introduction to Bayesian reasoning and methods" />
  <meta name="github-repo" content="rstudio/bayesian-reasoning-and-methods" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Considering Prior Distributions | An Introduction to Bayesian Reasoning and Methods" />
  
  <meta name="twitter:description" content="This textbook presents an introduction to Bayesian reasoning and methods" />
  

<meta name="author" content="Kevin Ross" />


<meta name="date" content="2021-01-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="example-17-1-beta-binomial-model-data-is-singular.html"/>
<link rel="next" href="model-comparison.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="interpretations.html"><a href="interpretations.html"><i class="fa fa-check"></i><b>1</b> Interpretations of Probability and Statistics</a><ul>
<li class="chapter" data-level="1.1" data-path="randomness.html"><a href="randomness.html"><i class="fa fa-check"></i><b>1.1</b> Instances of randomness</a></li>
<li class="chapter" data-level="1.2" data-path="interpretations-of-probability.html"><a href="interpretations-of-probability.html"><i class="fa fa-check"></i><b>1.2</b> Interpretations of probability</a><ul>
<li class="chapter" data-level="1.2.1" data-path="interpretations-of-probability.html"><a href="interpretations-of-probability.html#rel-freq"><i class="fa fa-check"></i><b>1.2.1</b> Long run relative frequency</a></li>
<li class="chapter" data-level="1.2.2" data-path="interpretations-of-probability.html"><a href="interpretations-of-probability.html#subjective-probability"><i class="fa fa-check"></i><b>1.2.2</b> Subjective probability</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="consistency.html"><a href="consistency.html"><i class="fa fa-check"></i><b>1.3</b> Working with probabilities</a><ul>
<li class="chapter" data-level="1.3.1" data-path="consistency.html"><a href="consistency.html#consistency-requirements"><i class="fa fa-check"></i><b>1.3.1</b> Consistency requirements</a></li>
<li class="chapter" data-level="1.3.2" data-path="consistency.html"><a href="consistency.html#odds"><i class="fa fa-check"></i><b>1.3.2</b> Odds</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="interpretations-of-statistics.html"><a href="interpretations-of-statistics.html"><i class="fa fa-check"></i><b>1.4</b> Interpretations of Statistics</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="bayes-rule.html"><a href="bayes-rule.html"><i class="fa fa-check"></i><b>2</b> Bayes’ Rule</a></li>
<li class="chapter" data-level="3" data-path="bayes-factor.html"><a href="bayes-factor.html"><i class="fa fa-check"></i><b>3</b> Odds and Bayes Factors</a></li>
<li class="chapter" data-level="4" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>4</b> Introduction to Estimation</a></li>
<li class="chapter" data-level="5" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>5</b> Introduction to Inference</a></li>
<li class="chapter" data-level="6" data-path="prediction.html"><a href="prediction.html"><i class="fa fa-check"></i><b>6</b> Introduction to Prediction</a></li>
<li class="chapter" data-level="7" data-path="continuous.html"><a href="continuous.html"><i class="fa fa-check"></i><b>7</b> Introduction to Continuous Prior and Posterior Distributions</a><ul>
<li class="chapter" data-level="7.1" data-path="a-brief-review-of-continuous-distributions.html"><a href="a-brief-review-of-continuous-distributions.html"><i class="fa fa-check"></i><b>7.1</b> A brief review of continuous distributions</a></li>
<li class="chapter" data-level="7.2" data-path="normal-mean.html"><a href="normal-mean.html"><i class="fa fa-check"></i><b>7.2</b> Normal mean</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="jags.html"><a href="jags.html"><i class="fa fa-check"></i><b>8</b> Introduction to Posterior Simulation and JAGS</a><ul>
<li class="chapter" data-level="8.1" data-path="implementing-mcmc-in-jags.html"><a href="implementing-mcmc-in-jags.html"><i class="fa fa-check"></i><b>8.1</b> Implementing MCMC in JAGS</a></li>
<li class="chapter" data-level="8.2" data-path="example-17-1-beta-binomial-model-data-is-singular.html"><a href="example-17-1-beta-binomial-model-data-is-singular.html"><i class="fa fa-check"></i><b>8.2</b> Example 17.1: Beta-Binomial Model — “Data is singular”</a><ul>
<li class="chapter" data-level="8.2.1" data-path="example-17-1-beta-binomial-model-data-is-singular.html"><a href="example-17-1-beta-binomial-model-data-is-singular.html#load-the-data"><i class="fa fa-check"></i><b>8.2.1</b> Load the data</a></li>
<li class="chapter" data-level="8.2.2" data-path="example-17-1-beta-binomial-model-data-is-singular.html"><a href="example-17-1-beta-binomial-model-data-is-singular.html#specify-the-model-likelihood-and-prior"><i class="fa fa-check"></i><b>8.2.2</b> Specify the model: likelihood and prior</a></li>
<li class="chapter" data-level="8.2.3" data-path="example-17-1-beta-binomial-model-data-is-singular.html"><a href="example-17-1-beta-binomial-model-data-is-singular.html#compile-in-jags"><i class="fa fa-check"></i><b>8.2.3</b> Compile in JAGS</a></li>
<li class="chapter" data-level="8.2.4" data-path="example-17-1-beta-binomial-model-data-is-singular.html"><a href="example-17-1-beta-binomial-model-data-is-singular.html#simulate-values-from-the-posterior-distribution"><i class="fa fa-check"></i><b>8.2.4</b> Simulate values from the posterior distribution</a></li>
<li class="chapter" data-level="8.2.5" data-path="example-17-1-beta-binomial-model-data-is-singular.html"><a href="example-17-1-beta-binomial-model-data-is-singular.html#summarizing-simulated-values-and-diagnostic-checking"><i class="fa fa-check"></i><b>8.2.5</b> Summarizing simulated values and diagnostic checking</a></li>
<li class="chapter" data-level="8.2.6" data-path="example-17-1-beta-binomial-model-data-is-singular.html"><a href="example-17-1-beta-binomial-model-data-is-singular.html#posterior-prediction"><i class="fa fa-check"></i><b>8.2.6</b> Posterior prediction</a></li>
<li class="chapter" data-level="8.2.7" data-path="example-17-1-beta-binomial-model-data-is-singular.html"><a href="example-17-1-beta-binomial-model-data-is-singular.html#loading-data-as-individual-values-rather-than-summary-statistics"><i class="fa fa-check"></i><b>8.2.7</b> Loading data as individual values rather than summary statistics</a></li>
<li class="chapter" data-level="8.2.8" data-path="example-17-1-beta-binomial-model-data-is-singular.html"><a href="example-17-1-beta-binomial-model-data-is-singular.html#simulating-multiple-chains"><i class="fa fa-check"></i><b>8.2.8</b> Simulating multiple chains</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="prior.html"><a href="prior.html"><i class="fa fa-check"></i><b>9</b> Considering Prior Distributions</a></li>
<li class="chapter" data-level="10" data-path="model-comparison.html"><a href="model-comparison.html"><i class="fa fa-check"></i><b>10</b> Introduction to Bayesian Model Comparison</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Bayesian Reasoning and Methods</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="prior" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Considering Prior Distributions</h1>
<p>Macalaseter example about why a prior is ok. Subjectivity - likelihood is also subjective</p>
<p>The question isn’t really “what is the prior?” but rather “is there a prior”?
That is, are you choosing a Bayesian approach and treating parameters as random variables.</p>
<p>There are advantages and disadvantages. Do STAT 305 Binomial vs Negative Binomial p-value problem here.</p>
<p>Tamika is a basketball player who has had a 0.7 probability of making any particular free throw throughout her career. Tamika has been practicing and believes now that her probability of making a free throw is greater than 0.7. Her coach agrees to allow Tamika to make 50 free throw attempts in an effort to convince
him that she has improved. Let the random variable <span class="math inline">\(X\)</span> represent the number of free throw attempts that Tamika successfully makes. Assume that shot attempts are independent, and the probability of success is the same on each attempt.</p>
<p>Suppose that Tamika has actually not improved and her probability of success is still 0.7. Determine how many shots (out of 50) Tamika would have to make, in order for the
probability of successfully making at least that many shots to be no greater than 0.025. (In statistical language, this determines the <code>rejection region'' for a level 0.025</code>significance’’ test.) The coach decides that if Tamika makes at least this many attempts, then he has convincing evidence that she has improved (because the probability of making this many attempts just by luck if she has not improved is fairly small, less than 0.025.) Compute the probability that Tamika actually makes this many attempts if her true probability of success on any attempt is 0.7.</p>
<p>If <span class="math inline">\(X\)</span> is the number of shots she makes, then <span class="math inline">\(X\)</span> has a Binomial(50, 0.7) distribution in this scenario. We want the smallest value <span class="math inline">\(x\)</span> which satisfies <span class="math inline">\(\IP(X \ge x) \le 0.025\)</span>. In other words, we want the largest <span class="math inline">\(x\)</span> for which <span class="math inline">\(\IP(X \le x - 1) \le 0.975\)</span>. Binomial(50, 0.7).quantile(1 - 0.025) returns 41; therefore <span class="math inline">\(x-1 = 41\)</span> so <span class="math inline">\(x = 42\)</span>. That is, she needs to make at least 42 shots. <span class="math inline">\(\IP(X \ge 42) = 0.018\)</span> from 1-Binomial(50, 0.7).cdf(41). (And <span class="math inline">\(\IP(X \ge 41)&gt;0.025\)</span>.)</p>
<p>Now suppose that Tamika has genuinely improved and her probability of success on any attempt is 0.8. Determine
the probability that she would successfully make enough shots (out of 50) to fall into the
rejection region from the previous part and therefore convince the coach that she has improved. (In statistical language, this is the <code>power'' of the level 0.025</code>significance’’ test.)</p>
<p>Now <span class="math inline">\(X\)</span> has a Binomial(50, 0.8) distribution. To emphasize that the distribution of <span class="math inline">\(X\)</span> has changed, but <span class="math inline">\(X\)</span> itself has not changed, we’ll write <span class="math inline">\(\textrm{Q}(X \ge 42)= 0.307\)</span> from 1 - Binomial(50, 0.8).cdf(41)</p>
<p>Redo parts a) and b) but now assuming that Tamika attempts 200 shots. Compare the results to those when <span class="math inline">\(n=50\)</span>. (Hint: compare values for <span class="math inline">\(X/n\)</span> between the two scenarios rather than <span class="math inline">\(X\)</span> itself.)</p>
<p>If she has not improved <span class="math inline">\(X\)</span> has a Binomial(200, 0.7) distribution and the rejection region is <span class="math inline">\(X \ge 153\)</span> with probability <span class="math inline">\(\IP(X \ge 153) = 0.0249\)</span> from 1 - Binomial(200, 0.7).cdf(152).</p>
<p>If she has improved <span class="math inline">\(X\)</span> has a Binomial(200, 0.8) distribution and <span class="math inline">\(\textrm{Q}(X \ge 153) = 0.905\)</span> from 1 - Binomial(200, 0.8).cdf(152).</p>
<p>We see that the probability of incorrectly concluding that she has improved when she hasn’t is about the same, 0.025, in both cases. But the probability of correctly concluding that she has improved when she really has is much larger when <span class="math inline">\(n=200\)</span> (the power is greater with the larger sample size).</p>
<p>Throughout her career Tamika has had a probability of 0.5 of making any three point attempt. However, her coach is afraid that Tamika has been focused so much on improving her free throw shooting that her three point shooting has gotten worse. To check this, the coach has Tamika shoot a series of three pointers; she makes 7 out of 24.</p>
<p>Suppose that Tamika has actually not gotten worse and her probability of success on any single attempt is still 0.5. Compute the probability that Tamika makes 7 or fewer attempts out of 24  (In statistical terms, this is called a ``<span class="math inline">\(p\)</span>-value.’’)</p>
<p>Let <span class="math inline">\(X\)</span> be the number of shots she makes. if she has not improved <span class="math inline">\(X\)</span> has a Binomial(24, 0.5) distribution, so the p-value is <span class="math inline">\(\IP(X \le 7) = 0.032\)</span> from Binomial(24, 0.5).cdf(7).</p>
<p>Suppose that Tamika has actually not gotten worse and her probability of success on any single attempt is still 0.5. Compute the probability that Tamika requires at least 24 attempts to make 7 three pointers  (In statistical terms, this is called a ``<span class="math inline">\(p\)</span>-value.’’)</p>
<p>Let <span class="math inline">\(Y\)</span> be the number of shots she makes. if she has not improved <span class="math inline">\(Y\)</span> has a NegativeBinomial(7, 0.5) distribution, so the p-value is <span class="math inline">\(\IP(Y \ge 24) = 0.017\)</span> from 1 - 1 - NegativeBinomial(7, 0.5).cdf(23)</p>
<p>Before Tamika makes her attempts, the coach decides that he has convincing evidence that Tamika has gotten worse if the <span class="math inline">\(p\)</span>-value is less than 0.025. (In statistical terms, 0.025 is called a ``significance level’’.) Does the fact that Tamika only made 7/24 attempts convince the coach that Tamika got worse? Explain. (The issue that you see here is one problem with using <span class="math inline">\(p\)</span>-values and significance levels. There are many, many more.)</p>
<p>It depends. If the coach had told her to attempt 24 attempts and she made 7, then the p-value of 0.03 is greater than the significance level of 0.025, so the coach does not have sufficient evidence that she got worse. If the coach had told her to shoot until shes makes 7 and it took her 24 shots, then the p-value of 0.017 is less than the significance level of 0.025, so the coach does have sufficient evidence that she got worse. But in either case she made 7/24 shots, so it seems undesirable that basically the same data could lead to two different conclusions. This is one problem with using <span class="math inline">\(p\)</span>-values and significance levels.</p>
<p>The prior distribution quantifies the researcher’s uncertainty about the parameters <em>before</em> observing data. Some issues to consider when choosing a prior include:</p>
<ul>
<li>The researcher’s prior beliefs! A prior distribution is part of a statistical model, and should be consistent with knowledge about the underlying scientific problem.</li>
<li>Informative or weakly informative prior. Prior is tuned to represent some subset of whatever actual prior knowledge is available, which might be expressed in terms of an ``equivalent prior sample size’’.</li>
<li>Noninformative prior a.k.a. (reference, vague, flat prior). A prior is sought that plays a minimal role in inference so that ``the data can speak for themselves’’.</li>
<li>Mathematical convenience. The prior is chosen so that computation of the posterior is simplified, as in the case of conjugate priors.</li>
<li>Prior based on past data. Bayesian updating can be viewed as an iterative process. The posterior distribution obtained from one round of data collection can inform the prior distribution for another round.</li>
</ul>
<p>This example illustrates some subtle issues that arise when attempting to choose a noninformative prior. Suppose we want to estimate <span class="math inline">\(\theta\)</span>, the population proportion of success for some binary variable. Add context</p>
<p>What are the possible values for <span class="math inline">\(\theta\)</span>. What prior distribution might you consider a noninformative prior distribution?</p>
<p>Recall how we interpreted the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Does the Beta(1, 1) distribution truly represent no prior information?</p>
<p>For a Beta(1, 1) prior, what is the posterior mean of <span class="math inline">\(\theta\)</span>. Does this let the data speak entirely for themselves?</p>
<p>How could you change <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to represent no prior information? Do you see any potential problems?</p>
<p>An <em>improper</em> prior distribution is a prior distribution that does not integrate to 1, so is not a proper probability density.</p>
<p>However, an improper proper often results in a proper posterior distribution. Thus, improper prior distributions are sometimes used in practice.</p>
<p>Assume a Beta(0, 0) prior for <span class="math inline">\(\theta\)</span>. In what cases is the posterior distribution a proper distribution? What is the posterior mean in these cases? The posterior mode?</p>
<p>Suppose the parameter you want to estimate is the <em>odds</em> of success <span class="math inline">\(\phi=\theta/(1-\theta)\)</span>. What are the possible values of <span class="math inline">\(\phi\)</span>? What might a non-informative prior look like?</p>
<p>Suppose that you assume a Beta(1, 1) prior for <span class="math inline">\(\theta\)</span>. What is the distribution of the odds <span class="math inline">\(\phi\)</span> under this prior? Would you say this is a noninformative prior for <span class="math inline">\(\phi\)</span>?</p>
<p>Some that <span class="math inline">\(\theta\)</span> represents the probability that an individual has a rare disease. Suppose that in <span class="math inline">\(n=100\)</span> suspected cases, none actually has the disease. Discuss the disadvantages of using a Beta(1, 1) prior in this case.</p>
<p>Just like the likelihood, the prior distribution is a part of a statistical model.
Perform sensitivity analysis to test the assumptions of the model, both for the form of the likelihood function and for the prior. For example, how does the inference change in response to changes in the prior distribution?
In general, in Bayesian estimation the larger the sample size the smaller the role that the prior plays.<br />
But it is often desirable for the prior to play some role.
We should not feel constrained to noninformative priors when prior knowledge is available.
Some subjectivity is OK!</p>
<p>What not to do</p>
<p>But choosing a prior based on current data — i.e., using the same data to determine both the prior and the likelihood — results in invalid inference.</p>
<p>Don’t assign 0 probability to possible values in prior - example</p>

</div>
            </section>

          </div>
        </div>
      </div>
<a href="example-17-1-beta-binomial-model-data-is-singular.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-comparison.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"],
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["bayesian-reasoning-and-methods.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
